

@Article{BerryMeister,
    author = {Berry, M J and Meister, M},
    title = {Refractoriness and neural precision},
    abstract = {The response of a spiking neuron to a stimulus is
        often characterized by its time-varying firing rate, estimated
            from a histogram of spike times. If the cell's firing
            probability in each small time interval depends only on
            this firing rate, one predicts a highly variable response
            to repeated trials, whereas many neurons show much greater
            fidelity. Furthermore, the neuronal membrane is refractory
            immediately after a spike, so that the firing probability
            depends not only on the stimulus but also on the preceding
            spike train. To connect these observations, we
            investigated the relationship between the refractory
            period of a neuron and its firing precision. The light
            response of retinal ganglion cells was modeled as
            probabilistic firing combined with a refractory period:
            the instantaneous firing rate is the product of a ``free
            firing rate, '' which depends only on the stimulus, and a
            ``recovery function,'' which depends only on the time
            since the last spike. This recovery function vanishes for
            an absolute refractory period and then gradually increases
            to unity. In simulations, longer refractory periods were
            found to make the response more reproducible, eventually
            matching the precision of measured spike trains.
            Refractoriness, although often thought to limit the
            performance of neurons, may in fact benefit neuronal
            reliability. The underlying free firing rate derived by
            allowing for the refractory period often exceeded the
            observed firing rate by an order of magnitude and was
            found to convey information about the stimulus over a much
            wider dynamic range. Thus, the free firing rate may be the
            preferred variable for describing the response of a
            spiking neuron.},
    journal = {J Neurosci},
    year = {1998},
    volume = {18},
    number = {6},
    pages = {2200-2211},
    month = {Mar},
    pmid = {9482804},
    url = {http://www.hubmed.org/display.cgi?uids=9482804},
}

''<++>

@book{DayanAbbott,
    abstract = {{Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.<br /> <br /> The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.}},
    author = {Dayan, Peter   and Abbott, L. F. },
    citeulike-article-id = {233773},
    howpublished = {Hardcover},
    isbn = {0262041995},
    keywords = {coding, method, strf},
    month = {December},
    posted-at = {2005-09-14 16:33:33},
    priority = {2},
    publisher = {{The MIT Press}},
    title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
    url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0262041995},
    year = {2001}
}

@Article{PaninskiHaithSzirtes,
author = "Paninski, L and Haith, A and Szirtes, G",
title = {Integral equation methods for computing likelihoods and their derivatives in the stochastic integrate-and-fire model},
abstract = {We recently introduced likelihood-based methods for fitting stochastic integrate-and-fire models to spike train data. The key component of this method involves the likelihood that the model will emit a spike at a given time t. Computing this likelihood is equivalent to computing a Markov first passage time density (the probability that the model voltage crosses threshold for the first time at time t). Here we detail an improved method for computing this likelihood, based on solving a certain integral equation. This integral equation method has several advantages over the techniques discussed in our previous work: in particular, the new method has fewer free parameters and is easily differentiable (for gradient computations). The new method is also easily adaptable for the case in which the model conductance, not just the input current, is time-varying. Finally, we describe how to incorporate large deviations approximations to very small likelihoods.},
journal = "J Comput Neurosci",
year = "2008",
volume = "24",
number = "1",
pages = "69-79",
month = "Feb",
pmid = "17492371",
url = "http://www.hubmed.org/display.cgi?uids=17492371",
doi = "10.1007/s10827-007-0042-x"
}

@Article{PaninskiPillowSimoncelli,
    author = {Paninski, L and Pillow, J W and Simoncelli, E P},
    title = {Maximum likelihood estimation of a stochastic
        integrate-and-fire neural encoding model},
    abstract = {We examine a cascade encoding model for neural
        response in which a linear filtering stage is followed by a
            noisy, leaky, integrate-and-fire spike generation
            mechanism. This model provides a biophysically more
            realistic alternative to models based on Poisson
            (memoryless) spike generation, and can effectively
            reproduce a variety of spiking behaviors seen in vivo. We
            describe the maximum likelihood estimator for the model
            parameters, given only extracellular spike train responses
            (not intracellular voltage data). Specifically, we prove
            that the log-likelihood function is concave and thus has
            an essentially unique global maximum that can be found
            using gradient ascent techniques. We develop an efficient
            algorithm for computing the maximum likelihood solution,
        demonstrate the effectiveness of the resulting estimator with
            numerical simulations, and discuss a method of testing the
            model's validity using time-rescaling and density
            evolution techniques.},
    journal = {Neural Comput},
    year = {2004},
    volume = {16},
    number = {12},
    pages = {2533-2561},
    month = {Dec},
    pmid = {15516273},
    url = {http://www.hubmed.org/display.cgi?uids=15516273},
    doi = {10.1162/0899766042321797}
}


